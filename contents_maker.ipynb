{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "1xjeL6QHiKx-YvWx4wTwKVVgVKyxGkxVs",
      "authorship_tag": "ABX9TyPgm3DXgZH+Rll1X4PeQ1N+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jodog0412/capstone_project/blob/main/contents_maker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "마케팅 사진을 만드는 코드입니다.  \n",
        "과정은 다음과 같습니다.\n",
        "1. chatGPT: 아이디어(input)->__마케팅 그림 묘사문__, 키워드\n",
        "2. Stable Diffusion: 키워드->__그림__\n",
        "3. Output: 그림(images)+묘사문(sentence)"
      ],
      "metadata": {
        "id": "37EAEQWtcmmU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQ_Cn3xowkgS"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade diffusers accelerate safetensors transformers openai xformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import torch\n",
        "from diffusers import DiffusionPipeline\n",
        "from safetensors.torch import load_file\n",
        "from collections import defaultdict\n",
        "from PIL import Image\n",
        "openai.api_key=\"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\" #본인의 openai api key를 입력해주세요!"
      ],
      "metadata": {
        "id": "5ecjjBBnYIWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input='making vegan burger'"
      ],
      "metadata": {
        "id": "0LLWRY9FcQjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer(prompt:str):\n",
        "        completion = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \n",
        "                \"content\": prompt}\n",
        "                ]\n",
        "        )\n",
        "        result=completion.choices[0].message[\"content\"]\n",
        "        return result\n",
        "\n",
        "class text_generation:\n",
        "    def __init__(self,input:str):\n",
        "        self.input=input\n",
        "        self.roleplay=f\"You're a marketing assistant.\\\n",
        "        Your task is to assist and make marketing contents from idea.\\\n",
        "        Given the idea delimated by '''.\\\n",
        "        Idea : '''{self.input}'''\"\n",
        "\n",
        "class idea(text_generation):\n",
        "    def keyword(self):\n",
        "        request=f\"Extract 3 keywords from the idea.\\\n",
        "        Write keywords using ',' symbol only. Using other symbols is not allowed.\"\n",
        "        \n",
        "        return answer(self.roleplay+request)\n",
        "\n",
        "    def introduction(self):\n",
        "        request=\"Perform the following actions:\\\n",
        "        1 - Create name for idea.\\\n",
        "        2 - Write catchphrase for idea.\\\n",
        "        Use the following format:\\\n",
        "        Name: <idea name>\\\n",
        "        Catchphrase: <idea catchphrase>\"\n",
        "\n",
        "        return answer(self.roleplay+request)\n",
        "    \n",
        "    def returns(self):\n",
        "        inputs=idea(self.input)\n",
        "\n",
        "        return inputs.introduction(), inputs.keyword()\n",
        "\n",
        "class content(text_generation):\n",
        "    def strategy(self):\n",
        "        request=\"Write the STP strategy for idea using macro and micro environment analysis.\\\n",
        "        Use the following format:\\\n",
        "        -Segmentation\\\n",
        "        ∙ Demographic: <demographic segmentation strategy>\\\n",
        "        ∙ Psychographic: <Psychographic segmentation strategy>\\\n",
        "        ∙ Behavioral: <Behavioral segmentation strategy>\\\n",
        "        -Targeting\\\n",
        "        <Targeting strategy>\\\n",
        "        -Positioning\\\n",
        "        <Positioning strategy>\"\n",
        "\n",
        "        return answer(self.roleplay+request)\n",
        "    \n",
        "    def introduction(self):\n",
        "        request=\"I want to make a picture for advertising idea, but I don't know what scene is suitable for marketing.\\\n",
        "        You can imagine a picture for idea. \\\n",
        "        Please describe the scene of photo for advertising idea in 4 lines.\"\n",
        "        sentence=answer(self.roleplay+request)\n",
        "        \n",
        "        keyword=answer(f\"I write the description for photo.\\\n",
        "        description is {sentence}.\\\n",
        "        Please extract and list 8 keywords for the scene of photo. \\\n",
        "        You should write keywords using ',' symbol only. Other symbols are not allowed.\")\n",
        "\n",
        "        return sentence, keyword\n",
        "    \n",
        "    def returns(self):\n",
        "        inputs=content(self.input)\n",
        "        return inputs.strategy(),inputs.introduction()"
      ],
      "metadata": {
        "id": "cJ7neOyd9SNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idea_txt,idea_key=idea(input).returns()"
      ],
      "metadata": {
        "id": "jogKiVwuu7HN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idea_txt,idea_key"
      ],
      "metadata": {
        "id": "e96co4KXX5UL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stp,(content_txt,content_key)=content(input).returns()"
      ],
      "metadata": {
        "id": "Npj39glyUWsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stp"
      ],
      "metadata": {
        "id": "xa6Qyti6X8vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(content_txt,content_key)"
      ],
      "metadata": {
        "id": "C1DOSktYX9Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_lora_weights(pipeline, checkpoint_path, multiplier, device, dtype):\n",
        "    LORA_PREFIX_UNET = \"lora_unet\"\n",
        "    LORA_PREFIX_TEXT_ENCODER = \"lora_te\"\n",
        "    # load LoRA weight from .safetensors\n",
        "    state_dict = load_file(checkpoint_path, device=device)\n",
        "\n",
        "    updates = defaultdict(dict)\n",
        "    for key, value in state_dict.items():\n",
        "        # it is suggested to print out the key, it usually will be something like below\n",
        "        # \"lora_te_text_model_encoder_layers_0_self_attn_k_proj.lora_down.weight\"\n",
        "\n",
        "        layer, elem = key.split('.', 1)\n",
        "        updates[layer][elem] = value\n",
        "\n",
        "    # directly update weight in diffusers model\n",
        "    for layer, elems in updates.items():\n",
        "\n",
        "        if \"text\" in layer:\n",
        "            layer_infos = layer.split(LORA_PREFIX_TEXT_ENCODER + \"_\")[-1].split(\"_\")\n",
        "            curr_layer = pipeline.text_encoder\n",
        "        else:\n",
        "            layer_infos = layer.split(LORA_PREFIX_UNET + \"_\")[-1].split(\"_\")\n",
        "            curr_layer = pipeline.unet\n",
        "\n",
        "        # find the target layer\n",
        "        temp_name = layer_infos.pop(0)\n",
        "        while len(layer_infos) > -1:\n",
        "            try:\n",
        "                curr_layer = curr_layer.__getattr__(temp_name)\n",
        "                if len(layer_infos) > 0:\n",
        "                    temp_name = layer_infos.pop(0)\n",
        "                elif len(layer_infos) == 0:\n",
        "                    break\n",
        "            except Exception:\n",
        "                if len(temp_name) > 0:\n",
        "                    temp_name += \"_\" + layer_infos.pop(0)\n",
        "                else:\n",
        "                    temp_name = layer_infos.pop(0)\n",
        "\n",
        "        # get elements for this layer\n",
        "        weight_up = elems['lora_up.weight'].to(dtype)\n",
        "        weight_down = elems['lora_down.weight'].to(dtype)\n",
        "        alpha = elems['alpha']\n",
        "        if alpha:\n",
        "            alpha = alpha.item() / weight_up.shape[1]\n",
        "        else:\n",
        "            alpha = 1.0\n",
        "\n",
        "        # update weight\n",
        "        if len(weight_up.shape) == 4:\n",
        "            curr_layer.weight.data += multiplier * alpha * torch.mm(weight_up.squeeze(3).squeeze(2), weight_down.squeeze(3).squeeze(2)).unsqueeze(2).unsqueeze(3)\n",
        "        else:\n",
        "            curr_layer.weight.data += multiplier * alpha * torch.mm(weight_up, weight_down)\n",
        "\n",
        "    return pipeline"
      ],
      "metadata": {
        "id": "ZUtg_JQ92I08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class image_generation:\n",
        "    def __init__(self,input:str):\n",
        "        self.input=input\n",
        "        repo_id=\"SG161222/Realistic_Vision_V2.0\"\n",
        "        pipeline=DiffusionPipeline.from_pretrained(repo_id, torch_dtype=torch.float16).to(\"cuda\")\n",
        "        pipeline.enable_xformers_memory_efficient_attention()\n",
        "        self.pipeline=pipeline\n",
        "\n",
        "def idea_image(input):\n",
        "    initial=image_generation(input)\n",
        "    pipeline=initial.pipeline\n",
        "    load_lora_weights(pipeline, \n",
        "                      checkpoint_path=\"/content/drive/MyDrive/Colab Notebooks/project/capstone_project/logo_v1-000012.safetensors\", \n",
        "                      multiplier=0.35, \n",
        "                      device='cuda', \n",
        "                      dtype=torch.float16)\n",
        "    pos=\"(masterpiece:1.2), best quality, high resolution, logo, 2d, white background, simple background\"\n",
        "    neg=\"(worst quality, low quality:1.4), 3d\"\n",
        "    images = pipeline(num_inference_steps=30, \n",
        "                      prompt=pos+','+input,\n",
        "                      negative_prompt=neg,\n",
        "                      num_images_per_prompt=8).images\n",
        "    return images\n",
        "    \n",
        "def content_image(input):\n",
        "    initial=image_generation(input)\n",
        "    pipeline=initial.pipeline\n",
        "    pos=\"masterpiece, raw photo, extremely detailed skin, extremely detailed face, 8k uhd, \\\n",
        "        dslr, soft lighting,  film grain, Fujifilm XT3, instagram, realstic\"\n",
        "    neg=\"(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime, mutated hands and fingers:1.4), \\\n",
        "        (deformed, distorted, disfigured:1.3), poorly drawn, bad anatomy, wrong anatomy, extra limb, missing limb, floating limbs, \\\n",
        "            disconnected limbs, mutation, mutated, ugly, disgusting, amputation,\\\n",
        "                (nsfw:2), (sexual content:2), (nude:2)\"\n",
        "    images = pipeline(num_inference_steps=30, \n",
        "                      prompt=pos+','+input,\n",
        "                      negative_prompt=neg,\n",
        "                      num_images_per_prompt=8).images\n",
        "    return images"
      ],
      "metadata": {
        "id": "xe-Iu-B0Dxve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logo=idea_image(idea_key)"
      ],
      "metadata": {
        "id": "CjZTHxj-4Q7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_img=content_image(content_key)"
      ],
      "metadata": {
        "id": "AfHO7jAjE6oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class output_process():\n",
        "    def __init__(self,image,sentence):\n",
        "        def image_grid(imgs, rows=2, cols=4):\n",
        "            assert len(imgs) == rows*cols\n",
        "\n",
        "            w, h = imgs[0].size\n",
        "            grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "            \n",
        "            for i, img in enumerate(imgs):\n",
        "                grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "            return grid\n",
        "        self.image=image\n",
        "        self.grid=image_grid(image)\n",
        "        self.sentence=sentence\n",
        "        \n",
        "    def returns(self):\n",
        "        return self.grid, self.sentence\n",
        "\n",
        "    def show(self):\n",
        "        self.grid.show()\n",
        "        print(self.sentence)"
      ],
      "metadata": {
        "id": "XK3fV8piIVud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# idea_logo,idea=output_process(logo,idea_txt).returns()"
      ],
      "metadata": {
        "id": "njI1_Yn849Ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_process(logo,idea_txt).show()"
      ],
      "metadata": {
        "id": "bLP_oy3A5rT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# content_grid,sentence=output_process(content_img,content_txt).returns()"
      ],
      "metadata": {
        "id": "NYMBetoqJZaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_process(content_img,content_txt).show()"
      ],
      "metadata": {
        "id": "ct3P9C2ZJPir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "deDQyURyJWfj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}